{
  "nodes": [
    {
      "id": "supervisor_0",
      "position": {
        "x": -1850.5716048691042,
        "y": -385.34651753742787
      },
      "type": "customNode",
      "data": {
        "id": "supervisor_0",
        "label": "Supervisor",
        "version": 3,
        "name": "supervisor",
        "type": "Supervisor",
        "baseClasses": [
          "Supervisor"
        ],
        "category": "Multi Agents",
        "inputParams": [
          {
            "label": "Supervisor Name",
            "name": "supervisorName",
            "type": "string",
            "placeholder": "Supervisor",
            "default": "Supervisor",
            "id": "supervisor_0-input-supervisorName-string"
          },
          {
            "label": "Supervisor Prompt",
            "name": "supervisorPrompt",
            "type": "string",
            "description": "Prompt must contains {team_members}",
            "rows": 4,
            "default": "You are a supervisor tasked with managing a conversation between the following workers: {team_members}.\nGiven the following user request, respond with the worker to act next.\nEach worker will perform a task and respond with their results and status.\nWhen finished, respond with FINISH.\nSelect strategically to minimize the number of steps taken.",
            "additionalParams": true,
            "id": "supervisor_0-input-supervisorPrompt-string"
          },
          {
            "label": "Summarization",
            "name": "summarization",
            "type": "boolean",
            "description": "Return final output as a summarization of the conversation",
            "optional": true,
            "additionalParams": true,
            "id": "supervisor_0-input-summarization-boolean"
          },
          {
            "label": "Recursion Limit",
            "name": "recursionLimit",
            "type": "number",
            "description": "Maximum number of times a call can recurse. If not provided, defaults to 100.",
            "default": 100,
            "additionalParams": true,
            "id": "supervisor_0-input-recursionLimit-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Tool Calling Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "description": "Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, GroqChat. Best result with GPT-4 model",
            "id": "supervisor_0-input-model-BaseChatModel"
          },
          {
            "label": "Agent Memory",
            "name": "agentMemory",
            "type": "BaseCheckpointSaver",
            "description": "Save the state of the agent",
            "optional": true,
            "id": "supervisor_0-input-agentMemory-BaseCheckpointSaver"
          },
          {
            "label": "Input Moderation",
            "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
            "name": "inputModeration",
            "type": "Moderation",
            "optional": true,
            "list": true,
            "id": "supervisor_0-input-inputModeration-Moderation"
          }
        ],
        "inputs": {
          "supervisorName": "Supervisor",
          "supervisorPrompt": "You are a tech research supervisor responsible for directing a team of analysts, a researcher and a summerizer. You have access to a few endpoints based on what the question is via different analysts. \n\nThe first thing you always want to do is ask the tech_researcher what the steps that should be based on a question that is coming from the user and based on the previous conversation, summarize and then give the researcher the question on what should be done from here. \n\nFrom then you can delegate what can be done to 4 different analysts that have access to different data. You need to make sure you send the correct information, such as the specific keywords and/or time period the researcher gave. If you do not do this then it will not be able to make the correct analysis.\n\nLastly when you have gathered all the information you will use the summarizing analyst to send it to, so it can create a story of it for the user. Make sure you send all statics and sources (texts and links) to this agent so they can do their job correctly. \n\nIn total you have access to the following workers: {team_members}. \n\nGiven the following user request, respond with the worker to act next.\n\nEach worker will perform a task and respond with their results and status. You should always go to the tech researcher to get an idea what the next step should be and to understand better which worker that should act next.\nWhen finished, respond with FINISH.\nSelect strategically to minimize the number of steps taken.\nEnd your tasks by responding with \"FINISH\".\n\n\n\n\n\n\n\n",
          "model": "{{chatOllama_0.data.instance}}",
          "agentMemory": "{{agentMemory_0.data.instance}}",
          "summarization": true,
          "recursionLimit": "100",
          "inputModeration": ""
        },
        "outputAnchors": [
          {
            "id": "supervisor_0-output-supervisor-Supervisor",
            "name": "supervisor",
            "label": "Supervisor",
            "description": "",
            "type": "Supervisor"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 483,
      "selected": false,
      "positionAbsolute": {
        "x": -1850.5716048691042,
        "y": -385.34651753742787
      },
      "dragging": false
    },
    {
      "id": "worker_0",
      "position": {
        "x": -438.0165558619117,
        "y": 166.05214648270265
      },
      "type": "customNode",
      "data": {
        "id": "worker_0",
        "label": "Worker",
        "version": 2,
        "name": "worker",
        "type": "Worker",
        "baseClasses": [
          "Worker"
        ],
        "category": "Multi Agents",
        "inputParams": [
          {
            "label": "Worker Name",
            "name": "workerName",
            "type": "string",
            "placeholder": "Worker",
            "id": "worker_0-input-workerName-string"
          },
          {
            "label": "Worker Prompt",
            "name": "workerPrompt",
            "type": "string",
            "rows": 4,
            "default": "You are a research assistant who can search for up-to-date info using search engine.",
            "id": "worker_0-input-workerPrompt-string"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "worker_0-input-promptValues-json"
          },
          {
            "label": "Max Iterations",
            "name": "maxIterations",
            "type": "number",
            "optional": true,
            "id": "worker_0-input-maxIterations-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Tools",
            "name": "tools",
            "type": "Tool",
            "list": true,
            "optional": true,
            "id": "worker_0-input-tools-Tool"
          },
          {
            "label": "Supervisor",
            "name": "supervisor",
            "type": "Supervisor",
            "id": "worker_0-input-supervisor-Supervisor"
          },
          {
            "label": "Tool Calling Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "optional": true,
            "description": "Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat. If not specified, supervisor's model will be used",
            "id": "worker_0-input-model-BaseChatModel"
          }
        ],
        "inputs": {
          "workerName": "Trending Keywords Analyst",
          "workerPrompt": "You are an assistant tasked with fetching trending and top keywords in tech for specified periods ('daily', 'weekly', 'monthly', or 'quarterly') using the \"safron_keywords\" tool. You do not fabricate information but simply gather data from the tool. You never add on information that has not been provided. Execute your tasks as follows:\n\nCategory Requests: When you receive a request specifying a category (like \"tools\" or \"platforms\"), make a single API call for that category to retrieve all relevant keywords. This call will provide a comprehensive list of keywords for the category, eliminating the need for subsequent keyword-specific calls.\n\nSpecific Keyword Requests: If the request specifically mentions a keyword, use that keyword to make a direct call to fetch its data. This ensures that you are only gathering information specific to that keyword without fetching broad category data first.\n\nCategories include:\n\n\"companies\": Companies & Organizations, e.g., Tesla, OpenAI.\n\"tools\": Tools & Services, e.g., ECS,",
          "tools": [
            "{{customTool_0.data.instance}}"
          ],
          "supervisor": "{{supervisor_0.data.instance}}",
          "model": "{{chatOllama_1.data.instance}}",
          "promptValues": "",
          "maxIterations": "10"
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 722,
      "selected": false,
      "positionAbsolute": {
        "x": -438.0165558619117,
        "y": 166.05214648270265
      },
      "dragging": false
    },
    {
      "id": "customTool_0",
      "position": {
        "x": -551.111661345928,
        "y": -708.1619542002813
      },
      "type": "customNode",
      "data": {
        "id": "customTool_0",
        "label": "Custom Tool",
        "version": 2,
        "name": "customTool",
        "type": "CustomTool",
        "baseClasses": [
          "CustomTool",
          "Tool",
          "StructuredTool",
          "Runnable"
        ],
        "category": "Tools",
        "description": "Use custom tool you've created in Flowise within chatflow",
        "inputParams": [
          {
            "label": "Select Tool",
            "name": "selectedTool",
            "type": "asyncOptions",
            "loadMethod": "listTools",
            "id": "customTool_0-input-selectedTool-asyncOptions"
          },
          {
            "label": "Return Direct",
            "name": "returnDirect",
            "description": "Return the output of the tool directly to the user",
            "type": "boolean",
            "optional": true,
            "id": "customTool_0-input-returnDirect-boolean"
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "selectedTool": "a37cf0df-6a1e-4a76-a9ea-59badf5a0751",
          "returnDirect": ""
        },
        "outputAnchors": [
          {
            "id": "customTool_0-output-customTool-CustomTool|Tool|StructuredTool|Runnable",
            "name": "customTool",
            "label": "CustomTool",
            "description": "Use custom tool you've created in Flowise within chatflow",
            "type": "CustomTool | Tool | StructuredTool | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 372,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": -551.111661345928,
        "y": -708.1619542002813
      }
    },
    {
      "id": "agentMemory_0",
      "position": {
        "x": -907.5867721225641,
        "y": -670.0438162450529
      },
      "type": "customNode",
      "data": {
        "id": "agentMemory_0",
        "label": "Agent Memory",
        "version": 1,
        "name": "agentMemory",
        "type": "AgentMemory",
        "baseClasses": [
          "AgentMemory",
          "BaseCheckpointSaver"
        ],
        "category": "Memory",
        "description": "Memory for agentflow to remember the state of the conversation",
        "inputParams": [
          {
            "label": "Database",
            "name": "databaseType",
            "type": "options",
            "options": [
              {
                "label": "SQLite",
                "name": "sqlite"
              }
            ],
            "default": "sqlite",
            "id": "agentMemory_0-input-databaseType-options"
          },
          {
            "label": "Database File Path",
            "name": "databaseFilePath",
            "type": "string",
            "placeholder": "C:\\Users\\User\\.flowise\\database.sqlite",
            "description": "If SQLite is selected, provide the path to the SQLite database file. Leave empty to use default application database",
            "additionalParams": true,
            "optional": true,
            "id": "agentMemory_0-input-databaseFilePath-string"
          },
          {
            "label": "Additional Connection Configuration",
            "name": "additionalConfig",
            "type": "json",
            "additionalParams": true,
            "optional": true,
            "id": "agentMemory_0-input-additionalConfig-json"
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "databaseType": "sqlite",
          "databaseFilePath": "",
          "additionalConfig": ""
        },
        "outputAnchors": [
          {
            "id": "agentMemory_0-output-agentMemory-AgentMemory|BaseCheckpointSaver",
            "name": "agentMemory",
            "label": "AgentMemory",
            "description": "Memory for agentflow to remember the state of the conversation",
            "type": "AgentMemory | BaseCheckpointSaver"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 328,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": -907.5867721225641,
        "y": -670.0438162450529
      }
    },
    {
      "id": "worker_2",
      "position": {
        "x": -65.83624711953692,
        "y": 166.32646912665962
      },
      "type": "customNode",
      "data": {
        "id": "worker_2",
        "label": "Worker",
        "version": 2,
        "name": "worker",
        "type": "Worker",
        "baseClasses": [
          "Worker"
        ],
        "category": "Multi Agents",
        "inputParams": [
          {
            "label": "Worker Name",
            "name": "workerName",
            "type": "string",
            "placeholder": "Worker",
            "id": "worker_2-input-workerName-string"
          },
          {
            "label": "Worker Prompt",
            "name": "workerPrompt",
            "type": "string",
            "rows": 4,
            "default": "You are a research assistant who can search for up-to-date info using search engine.",
            "id": "worker_2-input-workerPrompt-string"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "worker_2-input-promptValues-json"
          },
          {
            "label": "Max Iterations",
            "name": "maxIterations",
            "type": "number",
            "optional": true,
            "id": "worker_2-input-maxIterations-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Tools",
            "name": "tools",
            "type": "Tool",
            "list": true,
            "optional": true,
            "id": "worker_2-input-tools-Tool"
          },
          {
            "label": "Supervisor",
            "name": "supervisor",
            "type": "Supervisor",
            "id": "worker_2-input-supervisor-Supervisor"
          },
          {
            "label": "Tool Calling Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "optional": true,
            "description": "Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat. If not specified, supervisor's model will be used",
            "id": "worker_2-input-model-BaseChatModel"
          }
        ],
        "inputs": {
          "workerName": "Analyze Texts Analyst",
          "workerPrompt": "You are an assistant that will use the tool \"safron_sources\" to get back exact sources (texts and links) for each keyword. Remember to use the exact keyword given and no other versions of it, as that will give you limited results. If you do not get back any results make sure you've used the correct time period and then try again. You may not under any circumstances fabricate sources. If you are getting back an error or no resuls then you will simply say that you could not fetch the sources for the keyword(s).\n\nYou may use the \"safron_sources\" tool as many times as needed, for as many keywords that have been provided. If you do not get back any results make sure you've used the correct time period and then try again. Try until you have gotten the sources from the \"safron_sources\" tool. Do not fabricate sources that does not exist.",
          "tools": [
            "{{customTool_5.data.instance}}"
          ],
          "supervisor": "{{supervisor_0.data.instance}}",
          "model": "{{chatOllama_1.data.instance}}",
          "promptValues": "",
          "maxIterations": "10"
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 722,
      "selected": false,
      "positionAbsolute": {
        "x": -65.83624711953692,
        "y": 166.32646912665962
      },
      "dragging": false
    },
    {
      "id": "worker_3",
      "position": {
        "x": -1200.4201826782705,
        "y": 160.47994138443113
      },
      "type": "customNode",
      "data": {
        "id": "worker_3",
        "label": "Worker",
        "version": 2,
        "name": "worker",
        "type": "Worker",
        "baseClasses": [
          "Worker"
        ],
        "category": "Multi Agents",
        "inputParams": [
          {
            "label": "Worker Name",
            "name": "workerName",
            "type": "string",
            "placeholder": "Worker",
            "id": "worker_3-input-workerName-string"
          },
          {
            "label": "Worker Prompt",
            "name": "workerPrompt",
            "type": "string",
            "rows": 4,
            "default": "You are a research assistant who can search for up-to-date info using search engine.",
            "id": "worker_3-input-workerPrompt-string"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "worker_3-input-promptValues-json"
          },
          {
            "label": "Max Iterations",
            "name": "maxIterations",
            "type": "number",
            "optional": true,
            "id": "worker_3-input-maxIterations-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Tools",
            "name": "tools",
            "type": "Tool",
            "list": true,
            "optional": true,
            "id": "worker_3-input-tools-Tool"
          },
          {
            "label": "Supervisor",
            "name": "supervisor",
            "type": "Supervisor",
            "id": "worker_3-input-supervisor-Supervisor"
          },
          {
            "label": "Tool Calling Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "optional": true,
            "description": "Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat. If not specified, supervisor's model will be used",
            "id": "worker_3-input-model-BaseChatModel"
          }
        ],
        "inputs": {
          "workerName": "Connected Keywords Analyst",
          "workerPrompt": "You will be given one or several keywords, your task is to use the \"fetch_connected_keywords\" tool for each keyword  to get back statistics around trending and top keywords for a time period. It is important that you use the specific keywords when using the \"fetch_connected_keywords\" tool, so if the supervisor is giving you the keyword \"ChatGPT\" and \"Claude\" you use those keywords and not just \"AI\" as that will not get you back results for \"ChatGPT\" and \"Claude\" specifically.\n\nYou can query this tool as much as you need to, to get an understanding of the top and trending keywords connected to the keyword. But don't over use it, if you get two keywords use it two times. Always use the tool for each keyword and not together. I.e. if you get the keywords \"AWS, Elon Musk\" you will use the tool seperately on \"AWS\" and then again on \"Elon Musk\" and then send back data. \n\nNever fabricate information, statistics or keywords but always use the \"fetch_connected_keywords\" tool to get correct data for the specific keywor",
          "tools": [
            "{{customTool_3.data.instance}}"
          ],
          "supervisor": "{{supervisor_0.data.instance}}",
          "model": "{{chatOllama_1.data.instance}}",
          "promptValues": "",
          "maxIterations": "10"
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 722,
      "selected": false,
      "positionAbsolute": {
        "x": -1200.4201826782705,
        "y": 160.47994138443113
      },
      "dragging": false
    },
    {
      "id": "customTool_3",
      "position": {
        "x": -547.7080771238934,
        "y": -289.94704845711317
      },
      "type": "customNode",
      "data": {
        "id": "customTool_3",
        "label": "Custom Tool",
        "version": 2,
        "name": "customTool",
        "type": "CustomTool",
        "baseClasses": [
          "CustomTool",
          "Tool",
          "StructuredTool",
          "Runnable"
        ],
        "category": "Tools",
        "description": "Use custom tool you've created in Flowise within chatflow",
        "inputParams": [
          {
            "label": "Select Tool",
            "name": "selectedTool",
            "type": "asyncOptions",
            "loadMethod": "listTools",
            "id": "customTool_3-input-selectedTool-asyncOptions"
          },
          {
            "label": "Return Direct",
            "name": "returnDirect",
            "description": "Return the output of the tool directly to the user",
            "type": "boolean",
            "optional": true,
            "id": "customTool_3-input-returnDirect-boolean"
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "selectedTool": "401d22e4-fa19-42ff-b460-fbbbbe1cd988",
          "returnDirect": ""
        },
        "outputAnchors": [
          {
            "id": "customTool_3-output-customTool-CustomTool|Tool|StructuredTool|Runnable",
            "name": "customTool",
            "label": "CustomTool",
            "description": "Use custom tool you've created in Flowise within chatflow",
            "type": "CustomTool | Tool | StructuredTool | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 372,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": -547.7080771238934,
        "y": -289.94704845711317
      }
    },
    {
      "id": "worker_4",
      "position": {
        "x": -830.8129500005897,
        "y": 160.96578868887457
      },
      "type": "customNode",
      "data": {
        "id": "worker_4",
        "label": "Worker",
        "version": 2,
        "name": "worker",
        "type": "Worker",
        "baseClasses": [
          "Worker"
        ],
        "category": "Multi Agents",
        "inputParams": [
          {
            "label": "Worker Name",
            "name": "workerName",
            "type": "string",
            "placeholder": "Worker",
            "id": "worker_4-input-workerName-string"
          },
          {
            "label": "Worker Prompt",
            "name": "workerPrompt",
            "type": "string",
            "rows": 4,
            "default": "You are a research assistant who can search for up-to-date info using search engine.",
            "id": "worker_4-input-workerPrompt-string"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "worker_4-input-promptValues-json"
          },
          {
            "label": "Max Iterations",
            "name": "maxIterations",
            "type": "number",
            "optional": true,
            "id": "worker_4-input-maxIterations-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Tools",
            "name": "tools",
            "type": "Tool",
            "list": true,
            "optional": true,
            "id": "worker_4-input-tools-Tool"
          },
          {
            "label": "Supervisor",
            "name": "supervisor",
            "type": "Supervisor",
            "id": "worker_4-input-supervisor-Supervisor"
          },
          {
            "label": "Tool Calling Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "optional": true,
            "description": "Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat. If not specified, supervisor's model will be used",
            "id": "worker_4-input-model-BaseChatModel"
          }
        ],
        "inputs": {
          "workerName": "Graphing Keywords Analyst",
          "workerPrompt": "You are an assistant that is tasked with getting back keyword statistics via the \"graphing_keywords\" tool. This tool can be used to get back statistics for a keyword over time. You will be given keyword(s) and a time period and you are thus tasked with using the \"graphing_keywords\" tool for each keyword once each and then handing back the data that has been given. \n\nYou may use the \"graphing_keywords\" tool as much as you need to, but do not iterate for the same keyword over and over once you have the data you need. If you only have one keyword, then you only need to call  \"graphing_keywords\" tool once successfully.\n\nThe time periods will aggregate data differently, the daily will get you back data by day, the weekly will get you data back per 3 days, the monthly will get you back data by week and quarterly will get you data back by every two weeks. Make sure you communicate this clearly with the supervisor.\n\n",
          "tools": [
            "{{customTool_4.data.instance}}"
          ],
          "supervisor": "{{supervisor_0.data.instance}}",
          "model": "{{chatOllama_1.data.instance}}",
          "promptValues": "",
          "maxIterations": "20"
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 722,
      "selected": false,
      "positionAbsolute": {
        "x": -830.8129500005897,
        "y": 160.96578868887457
      },
      "dragging": false
    },
    {
      "id": "customTool_4",
      "position": {
        "x": -197.87603694026097,
        "y": -700.7865865263457
      },
      "type": "customNode",
      "data": {
        "id": "customTool_4",
        "label": "Custom Tool",
        "version": 2,
        "name": "customTool",
        "type": "CustomTool",
        "baseClasses": [
          "CustomTool",
          "Tool",
          "StructuredTool",
          "Runnable"
        ],
        "category": "Tools",
        "description": "Use custom tool you've created in Flowise within chatflow",
        "inputParams": [
          {
            "label": "Select Tool",
            "name": "selectedTool",
            "type": "asyncOptions",
            "loadMethod": "listTools",
            "id": "customTool_4-input-selectedTool-asyncOptions"
          },
          {
            "label": "Return Direct",
            "name": "returnDirect",
            "description": "Return the output of the tool directly to the user",
            "type": "boolean",
            "optional": true,
            "id": "customTool_4-input-returnDirect-boolean"
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "selectedTool": "5d2530da-c39b-491c-81b4-696d5fb4ccb4",
          "returnDirect": ""
        },
        "outputAnchors": [
          {
            "id": "customTool_4-output-customTool-CustomTool|Tool|StructuredTool|Runnable",
            "name": "customTool",
            "label": "CustomTool",
            "description": "Use custom tool you've created in Flowise within chatflow",
            "type": "CustomTool | Tool | StructuredTool | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 372,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": -197.87603694026097,
        "y": -700.7865865263457
      }
    },
    {
      "id": "worker_5",
      "position": {
        "x": -1945.3566738977713,
        "y": 158.70591385746735
      },
      "type": "customNode",
      "data": {
        "id": "worker_5",
        "label": "Worker",
        "version": 2,
        "name": "worker",
        "type": "Worker",
        "baseClasses": [
          "Worker"
        ],
        "category": "Multi Agents",
        "inputParams": [
          {
            "label": "Worker Name",
            "name": "workerName",
            "type": "string",
            "placeholder": "Worker",
            "id": "worker_5-input-workerName-string"
          },
          {
            "label": "Worker Prompt",
            "name": "workerPrompt",
            "type": "string",
            "rows": 4,
            "default": "You are a research assistant who can search for up-to-date info using search engine.",
            "id": "worker_5-input-workerPrompt-string"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "worker_5-input-promptValues-json"
          },
          {
            "label": "Max Iterations",
            "name": "maxIterations",
            "type": "number",
            "optional": true,
            "id": "worker_5-input-maxIterations-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Tools",
            "name": "tools",
            "type": "Tool",
            "list": true,
            "optional": true,
            "id": "worker_5-input-tools-Tool"
          },
          {
            "label": "Supervisor",
            "name": "supervisor",
            "type": "Supervisor",
            "id": "worker_5-input-supervisor-Supervisor"
          },
          {
            "label": "Tool Calling Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "optional": true,
            "description": "Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat. If not specified, supervisor's model will be used",
            "id": "worker_5-input-model-BaseChatModel"
          }
        ],
        "inputs": {
          "workerName": "Tech Researcher",
          "workerPrompt": "You are a tech researcher, you are tasked with telling the supervisor what it should do with a question that is coming in from a user. A user can ask for things such as \"what is trending yesterday or this month\" or \"I want to explore RAG and what's new there\" or \"I'm in product management in tech I need an update.\" You are tasked with figuring out what steps the supervisor should take to achieve the correct goal. \n\nYou have 4 choices to work with. \n\n1. You can use the \"keyword_trends_analyst\", to get back statistics for a period (daily, weekly, monthly or quarterly) based on either a category (Companies & Organizations, Tools & Services, Platforms & Search Engines, Hardware & Systems, Frameworks & Libraries, Languages & Syntax, AI Models & Assistants, Websites & Applications, People, Subjects, Concepts & Methods, and Bucket (other)), search with a keyword (such as OpenAI or Open Source) or sentiment (negative, positive or neutral).\n\n2. You can use the \"analyze_texts_analyst\", to get back the exact sources (te",
          "tools": [],
          "supervisor": "{{supervisor_0.data.instance}}",
          "model": "",
          "promptValues": "",
          "maxIterations": "3"
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 722,
      "selected": false,
      "positionAbsolute": {
        "x": -1945.3566738977713,
        "y": 158.70591385746735
      },
      "dragging": false
    },
    {
      "id": "customTool_5",
      "position": {
        "x": -190.81209366314448,
        "y": -298.34872339355115
      },
      "type": "customNode",
      "data": {
        "id": "customTool_5",
        "label": "Custom Tool",
        "version": 2,
        "name": "customTool",
        "type": "CustomTool",
        "baseClasses": [
          "CustomTool",
          "Tool",
          "StructuredTool",
          "Runnable"
        ],
        "category": "Tools",
        "description": "Use custom tool you've created in Flowise within chatflow",
        "inputParams": [
          {
            "label": "Select Tool",
            "name": "selectedTool",
            "type": "asyncOptions",
            "loadMethod": "listTools",
            "id": "customTool_5-input-selectedTool-asyncOptions"
          },
          {
            "label": "Return Direct",
            "name": "returnDirect",
            "description": "Return the output of the tool directly to the user",
            "type": "boolean",
            "optional": true,
            "id": "customTool_5-input-returnDirect-boolean"
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "selectedTool": "0ea618bb-b58f-48a3-a305-56d3df719a83",
          "returnDirect": ""
        },
        "outputAnchors": [
          {
            "id": "customTool_5-output-customTool-CustomTool|Tool|StructuredTool|Runnable",
            "name": "customTool",
            "label": "CustomTool",
            "description": "Use custom tool you've created in Flowise within chatflow",
            "type": "CustomTool | Tool | StructuredTool | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 372,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": -190.81209366314448,
        "y": -298.34872339355115
      }
    },
    {
      "id": "worker_6",
      "position": {
        "x": -1590.462104719452,
        "y": 158.04098331521345
      },
      "type": "customNode",
      "data": {
        "id": "worker_6",
        "label": "Worker",
        "version": 2,
        "name": "worker",
        "type": "Worker",
        "baseClasses": [
          "Worker"
        ],
        "category": "Multi Agents",
        "inputParams": [
          {
            "label": "Worker Name",
            "name": "workerName",
            "type": "string",
            "placeholder": "Worker",
            "id": "worker_6-input-workerName-string"
          },
          {
            "label": "Worker Prompt",
            "name": "workerPrompt",
            "type": "string",
            "rows": 4,
            "default": "You are a research assistant who can search for up-to-date info using search engine.",
            "id": "worker_6-input-workerPrompt-string"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "worker_6-input-promptValues-json"
          },
          {
            "label": "Max Iterations",
            "name": "maxIterations",
            "type": "number",
            "optional": true,
            "id": "worker_6-input-maxIterations-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Tools",
            "name": "tools",
            "type": "Tool",
            "list": true,
            "optional": true,
            "id": "worker_6-input-tools-Tool"
          },
          {
            "label": "Supervisor",
            "name": "supervisor",
            "type": "Supervisor",
            "id": "worker_6-input-supervisor-Supervisor"
          },
          {
            "label": "Tool Calling Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "optional": true,
            "description": "Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat. If not specified, supervisor's model will be used",
            "id": "worker_6-input-model-BaseChatModel"
          }
        ],
        "inputs": {
          "workerName": "Summarizing Analyst",
          "workerPrompt": "You are a summarizing analyst, that will be the assistant who will summarize all the data that has been gathered so far for a user question. You do not simply output a bunch of data but you clearly summarize what the data says to tell a story for each keyword and as a whole. You will get back statistics and sources (texts and links) to be able to accurately summarize and present this information so it can be digested based on a user question. Remember to look at the sources and summarize for the user so they do not have to read each and every source on their own. You will never fabricate or make up statistics, sources and keywords but simply summarize the data you are provided with.",
          "tools": [],
          "supervisor": "{{supervisor_0.data.instance}}",
          "model": "",
          "promptValues": "",
          "maxIterations": "3"
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 722,
      "selected": false,
      "positionAbsolute": {
        "x": -1590.462104719452,
        "y": 158.04098331521345
      },
      "dragging": false
    },
    {
      "id": "chatOllama_0",
      "position": {
        "x": -2253.6617801322373,
        "y": -1101.7236225386837
      },
      "type": "customNode",
      "data": {
        "id": "chatOllama_0",
        "label": "ChatOllama",
        "version": 3,
        "name": "chatOllama",
        "type": "ChatOllama",
        "baseClasses": [
          "ChatOllama",
          "BaseChatModel",
          "BaseLanguageModel",
          "Runnable"
        ],
        "category": "Chat Models",
        "description": "Chat completion using open-source LLM on Ollama",
        "inputParams": [
          {
            "label": "Base URL",
            "name": "baseUrl",
            "type": "string",
            "default": "http://localhost:11434",
            "id": "chatOllama_0-input-baseUrl-string"
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "string",
            "placeholder": "llama2",
            "id": "chatOllama_0-input-modelName-string"
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "description": "The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "default": 0.9,
            "optional": true,
            "id": "chatOllama_0-input-temperature-number"
          },
          {
            "label": "Keep Alive",
            "name": "keepAlive",
            "type": "string",
            "description": "How long to keep connection alive. A duration string (such as \"10m\" or \"24h\")",
            "default": "5m",
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-keepAlive-string"
          },
          {
            "label": "Top P",
            "name": "topP",
            "type": "number",
            "description": "Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-topP-number"
          },
          {
            "label": "Top K",
            "name": "topK",
            "type": "number",
            "description": "Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-topK-number"
          },
          {
            "label": "Mirostat",
            "name": "mirostat",
            "type": "number",
            "description": "Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-mirostat-number"
          },
          {
            "label": "Mirostat ETA",
            "name": "mirostatEta",
            "type": "number",
            "description": "Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-mirostatEta-number"
          },
          {
            "label": "Mirostat TAU",
            "name": "mirostatTau",
            "type": "number",
            "description": "Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-mirostatTau-number"
          },
          {
            "label": "Context Window Size",
            "name": "numCtx",
            "type": "number",
            "description": "Sets the size of the context window used to generate the next token. (Default: 2048) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-numCtx-number"
          },
          {
            "label": "Number of GPU",
            "name": "numGpu",
            "type": "number",
            "description": "The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-numGpu-number"
          },
          {
            "label": "Number of Thread",
            "name": "numThread",
            "type": "number",
            "description": "Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-numThread-number"
          },
          {
            "label": "Repeat Last N",
            "name": "repeatLastN",
            "type": "number",
            "description": "Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-repeatLastN-number"
          },
          {
            "label": "Repeat Penalty",
            "name": "repeatPenalty",
            "type": "number",
            "description": "Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-repeatPenalty-number"
          },
          {
            "label": "Stop Sequence",
            "name": "stop",
            "type": "string",
            "rows": 4,
            "placeholder": "AI assistant:",
            "description": "Sets the stop sequences to use. Use comma to seperate different sequences. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-stop-string"
          },
          {
            "label": "Tail Free Sampling",
            "name": "tfsZ",
            "type": "number",
            "description": "Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (Default: 1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-tfsZ-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Cache",
            "name": "cache",
            "type": "BaseCache",
            "optional": true,
            "id": "chatOllama_0-input-cache-BaseCache"
          }
        ],
        "inputs": {
          "cache": "",
          "baseUrl": "http://host.docker.internal:11434",
          "modelName": "llama3.2:latest",
          "temperature": "0.2",
          "keepAlive": "5m",
          "topP": "",
          "topK": "",
          "mirostat": "",
          "mirostatEta": "",
          "mirostatTau": "",
          "numCtx": "",
          "numGpu": "",
          "numThread": "",
          "repeatLastN": "",
          "repeatPenalty": "",
          "stop": "",
          "tfsZ": ""
        },
        "outputAnchors": [
          {
            "id": "chatOllama_0-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable",
            "name": "chatOllama",
            "label": "ChatOllama",
            "description": "Chat completion using open-source LLM on Ollama",
            "type": "ChatOllama | BaseChatModel | BaseLanguageModel | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 580,
      "selected": false,
      "positionAbsolute": {
        "x": -2253.6617801322373,
        "y": -1101.7236225386837
      },
      "dragging": false
    },
    {
      "id": "chatOllama_1",
      "position": {
        "x": -1729.7944989624796,
        "y": -1088.7822648045299
      },
      "type": "customNode",
      "data": {
        "id": "chatOllama_1",
        "label": "ChatOllama",
        "version": 3,
        "name": "chatOllama",
        "type": "ChatOllama",
        "baseClasses": [
          "ChatOllama",
          "BaseChatModel",
          "BaseLanguageModel",
          "Runnable"
        ],
        "category": "Chat Models",
        "description": "Chat completion using open-source LLM on Ollama",
        "inputParams": [
          {
            "label": "Base URL",
            "name": "baseUrl",
            "type": "string",
            "default": "http://localhost:11434",
            "id": "chatOllama_1-input-baseUrl-string"
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "string",
            "placeholder": "llama2",
            "id": "chatOllama_1-input-modelName-string"
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "description": "The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "default": 0.9,
            "optional": true,
            "id": "chatOllama_1-input-temperature-number"
          },
          {
            "label": "Keep Alive",
            "name": "keepAlive",
            "type": "string",
            "description": "How long to keep connection alive. A duration string (such as \"10m\" or \"24h\")",
            "default": "5m",
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-keepAlive-string"
          },
          {
            "label": "Top P",
            "name": "topP",
            "type": "number",
            "description": "Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-topP-number"
          },
          {
            "label": "Top K",
            "name": "topK",
            "type": "number",
            "description": "Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-topK-number"
          },
          {
            "label": "Mirostat",
            "name": "mirostat",
            "type": "number",
            "description": "Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-mirostat-number"
          },
          {
            "label": "Mirostat ETA",
            "name": "mirostatEta",
            "type": "number",
            "description": "Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-mirostatEta-number"
          },
          {
            "label": "Mirostat TAU",
            "name": "mirostatTau",
            "type": "number",
            "description": "Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-mirostatTau-number"
          },
          {
            "label": "Context Window Size",
            "name": "numCtx",
            "type": "number",
            "description": "Sets the size of the context window used to generate the next token. (Default: 2048) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-numCtx-number"
          },
          {
            "label": "Number of GPU",
            "name": "numGpu",
            "type": "number",
            "description": "The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-numGpu-number"
          },
          {
            "label": "Number of Thread",
            "name": "numThread",
            "type": "number",
            "description": "Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-numThread-number"
          },
          {
            "label": "Repeat Last N",
            "name": "repeatLastN",
            "type": "number",
            "description": "Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-repeatLastN-number"
          },
          {
            "label": "Repeat Penalty",
            "name": "repeatPenalty",
            "type": "number",
            "description": "Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-repeatPenalty-number"
          },
          {
            "label": "Stop Sequence",
            "name": "stop",
            "type": "string",
            "rows": 4,
            "placeholder": "AI assistant:",
            "description": "Sets the stop sequences to use. Use comma to seperate different sequences. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-stop-string"
          },
          {
            "label": "Tail Free Sampling",
            "name": "tfsZ",
            "type": "number",
            "description": "Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (Default: 1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_1-input-tfsZ-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Cache",
            "name": "cache",
            "type": "BaseCache",
            "optional": true,
            "id": "chatOllama_1-input-cache-BaseCache"
          }
        ],
        "inputs": {
          "cache": "",
          "baseUrl": "http://host.docker.internal:11434",
          "modelName": "llama3.2:latest",
          "temperature": "0.2",
          "keepAlive": "5m",
          "topP": "",
          "topK": "",
          "mirostat": "",
          "mirostatEta": "",
          "mirostatTau": "",
          "numCtx": "",
          "numGpu": "",
          "numThread": "",
          "repeatLastN": "",
          "repeatPenalty": "",
          "stop": "",
          "tfsZ": ""
        },
        "outputAnchors": [
          {
            "id": "chatOllama_1-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable",
            "name": "chatOllama",
            "label": "ChatOllama",
            "description": "Chat completion using open-source LLM on Ollama",
            "type": "ChatOllama | BaseChatModel | BaseLanguageModel | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 580,
      "selected": false,
      "positionAbsolute": {
        "x": -1729.7944989624796,
        "y": -1088.7822648045299
      },
      "dragging": false
    },
    {
      "id": "stickyNote_0",
      "position": {
        "x": -1247.9269357263654,
        "y": -1115.772979498115
      },
      "type": "stickyNote",
      "data": {
        "id": "stickyNote_0",
        "label": "Sticky Note",
        "version": 2,
        "name": "stickyNote",
        "type": "StickyNote",
        "baseClasses": [
          "StickyNote"
        ],
        "tags": [
          "Utilities"
        ],
        "category": "Utilities",
        "description": "Add a sticky note",
        "inputParams": [
          {
            "label": "",
            "name": "note",
            "type": "string",
            "rows": 1,
            "placeholder": "Type something here",
            "optional": true,
            "id": "stickyNote_0-input-note-string"
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "note": "https://working-purple-9a3.notion.site/Agentic-AI-Building-of-a-Tech-Research-Agent-using-flowise-e13140f44bb14614aed97c40e15e0d18"
        },
        "outputAnchors": [
          {
            "id": "stickyNote_0-output-stickyNote-StickyNote",
            "name": "stickyNote",
            "label": "StickyNote",
            "description": "Add a sticky note",
            "type": "StickyNote"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 103,
      "selected": false,
      "dragging": false,
      "positionAbsolute": {
        "x": -1247.9269357263654,
        "y": -1115.772979498115
      }
    }
  ],
  "edges": [
    {
      "source": "supervisor_0",
      "sourceHandle": "supervisor_0-output-supervisor-Supervisor",
      "target": "worker_0",
      "targetHandle": "worker_0-input-supervisor-Supervisor",
      "type": "buttonedge",
      "id": "supervisor_0-supervisor_0-output-supervisor-Supervisor-worker_0-worker_0-input-supervisor-Supervisor"
    },
    {
      "source": "customTool_0",
      "sourceHandle": "customTool_0-output-customTool-CustomTool|Tool|StructuredTool|Runnable",
      "target": "worker_0",
      "targetHandle": "worker_0-input-tools-Tool",
      "type": "buttonedge",
      "id": "customTool_0-customTool_0-output-customTool-CustomTool|Tool|StructuredTool|Runnable-worker_0-worker_0-input-tools-Tool"
    },
    {
      "source": "agentMemory_0",
      "sourceHandle": "agentMemory_0-output-agentMemory-AgentMemory|BaseCheckpointSaver",
      "target": "supervisor_0",
      "targetHandle": "supervisor_0-input-agentMemory-BaseCheckpointSaver",
      "type": "buttonedge",
      "id": "agentMemory_0-agentMemory_0-output-agentMemory-AgentMemory|BaseCheckpointSaver-supervisor_0-supervisor_0-input-agentMemory-BaseCheckpointSaver"
    },
    {
      "source": "supervisor_0",
      "sourceHandle": "supervisor_0-output-supervisor-Supervisor",
      "target": "worker_2",
      "targetHandle": "worker_2-input-supervisor-Supervisor",
      "type": "buttonedge",
      "id": "supervisor_0-supervisor_0-output-supervisor-Supervisor-worker_2-worker_2-input-supervisor-Supervisor"
    },
    {
      "source": "supervisor_0",
      "sourceHandle": "supervisor_0-output-supervisor-Supervisor",
      "target": "worker_3",
      "targetHandle": "worker_3-input-supervisor-Supervisor",
      "type": "buttonedge",
      "id": "supervisor_0-supervisor_0-output-supervisor-Supervisor-worker_3-worker_3-input-supervisor-Supervisor"
    },
    {
      "source": "customTool_3",
      "sourceHandle": "customTool_3-output-customTool-CustomTool|Tool|StructuredTool|Runnable",
      "target": "worker_3",
      "targetHandle": "worker_3-input-tools-Tool",
      "type": "buttonedge",
      "id": "customTool_3-customTool_3-output-customTool-CustomTool|Tool|StructuredTool|Runnable-worker_3-worker_3-input-tools-Tool"
    },
    {
      "source": "supervisor_0",
      "sourceHandle": "supervisor_0-output-supervisor-Supervisor",
      "target": "worker_4",
      "targetHandle": "worker_4-input-supervisor-Supervisor",
      "type": "buttonedge",
      "id": "supervisor_0-supervisor_0-output-supervisor-Supervisor-worker_4-worker_4-input-supervisor-Supervisor"
    },
    {
      "source": "customTool_4",
      "sourceHandle": "customTool_4-output-customTool-CustomTool|Tool|StructuredTool|Runnable",
      "target": "worker_4",
      "targetHandle": "worker_4-input-tools-Tool",
      "type": "buttonedge",
      "id": "customTool_4-customTool_4-output-customTool-CustomTool|Tool|StructuredTool|Runnable-worker_4-worker_4-input-tools-Tool"
    },
    {
      "source": "supervisor_0",
      "sourceHandle": "supervisor_0-output-supervisor-Supervisor",
      "target": "worker_5",
      "targetHandle": "worker_5-input-supervisor-Supervisor",
      "type": "buttonedge",
      "id": "supervisor_0-supervisor_0-output-supervisor-Supervisor-worker_5-worker_5-input-supervisor-Supervisor"
    },
    {
      "source": "customTool_5",
      "sourceHandle": "customTool_5-output-customTool-CustomTool|Tool|StructuredTool|Runnable",
      "target": "worker_2",
      "targetHandle": "worker_2-input-tools-Tool",
      "type": "buttonedge",
      "id": "customTool_5-customTool_5-output-customTool-CustomTool|Tool|StructuredTool|Runnable-worker_2-worker_2-input-tools-Tool"
    },
    {
      "source": "supervisor_0",
      "sourceHandle": "supervisor_0-output-supervisor-Supervisor",
      "target": "worker_6",
      "targetHandle": "worker_6-input-supervisor-Supervisor",
      "type": "buttonedge",
      "id": "supervisor_0-supervisor_0-output-supervisor-Supervisor-worker_6-worker_6-input-supervisor-Supervisor"
    },
    {
      "source": "chatOllama_0",
      "sourceHandle": "chatOllama_0-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable",
      "target": "supervisor_0",
      "targetHandle": "supervisor_0-input-model-BaseChatModel",
      "type": "buttonedge",
      "id": "chatOllama_0-chatOllama_0-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable-supervisor_0-supervisor_0-input-model-BaseChatModel"
    },
    {
      "source": "chatOllama_1",
      "sourceHandle": "chatOllama_1-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable",
      "target": "worker_3",
      "targetHandle": "worker_3-input-model-BaseChatModel",
      "type": "buttonedge",
      "id": "chatOllama_1-chatOllama_1-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable-worker_3-worker_3-input-model-BaseChatModel"
    },
    {
      "source": "chatOllama_1",
      "sourceHandle": "chatOllama_1-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable",
      "target": "worker_4",
      "targetHandle": "worker_4-input-model-BaseChatModel",
      "type": "buttonedge",
      "id": "chatOllama_1-chatOllama_1-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable-worker_4-worker_4-input-model-BaseChatModel"
    },
    {
      "source": "chatOllama_1",
      "sourceHandle": "chatOllama_1-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable",
      "target": "worker_0",
      "targetHandle": "worker_0-input-model-BaseChatModel",
      "type": "buttonedge",
      "id": "chatOllama_1-chatOllama_1-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable-worker_0-worker_0-input-model-BaseChatModel"
    },
    {
      "source": "chatOllama_1",
      "sourceHandle": "chatOllama_1-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable",
      "target": "worker_2",
      "targetHandle": "worker_2-input-model-BaseChatModel",
      "type": "buttonedge",
      "id": "chatOllama_1-chatOllama_1-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable-worker_2-worker_2-input-model-BaseChatModel"
    }
  ]
}